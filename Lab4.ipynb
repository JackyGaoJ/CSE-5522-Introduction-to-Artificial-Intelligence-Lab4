{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4\n",
    "## Author : Jian Gao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Multilevel Perceptron with 2 layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self,input_dim, hid_dim, output_dim):\n",
    "        self.input_dim=input_dim\n",
    "        self.hid_dim=hid_dim\n",
    "        self.output_dim=output_dim\n",
    "        # initialze the superclass\n",
    "        super(MLP2, self).__init__()\n",
    "        # this will create a linear layer with input_dim x hid_dim parameters\n",
    "        # in addition to a bias unit with hid_dim parameters\n",
    "        self.lin1 = nn.Linear(input_dim, hid_dim)\n",
    "        # same thing here except hid_dim x output_dim\n",
    "        self.lin2 = nn.Linear(hid_dim, output_dim)\n",
    "    \n",
    "    # this is where the meat of the action is\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)  # linear combination of inputs\n",
    "        x = torch.sigmoid(x) # then through sigmoid - output of first layer\n",
    "        x = self.lin2(x) # linear combination of hidden units\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # initialize the weight tensor, here we use a normal distribution\n",
    "            m.weight.data.normal_(0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train function, and print loss every 1000 iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, target, epoch, inputDim, hiddenDim, outputDim):\n",
    "    model = MLP2(inputDim, hiddenDim, outputDim)\n",
    "    weights_init(model)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0006, momentum=0.9)\n",
    "    for i in range(epoch):\n",
    "        x_var = Variable(data, requires_grad=False)\n",
    "        y_var = Variable(target, requires_grad=False)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x_var.float())\n",
    "        y_var = y_var.long()\n",
    "        loss = loss_func(y_hat, y_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Epoch: {0}, Loss: {1}, \".format(i, loss.data.numpy()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Test function, and return the prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data, test_target, trained_model):\n",
    "    x_var = Variable(test_data, requires_grad=False)\n",
    "    y_var = Variable(test_target, requires_grad=False)\n",
    "    y_hat = trained_model(x_var.float())\n",
    "    y_var = y_var.long()\n",
    "    all_prediction_index = []\n",
    "    prediction = y_hat.detach().numpy()\n",
    "    for i in range (prediction.shape[0]):\n",
    "        all_prediction_index.append(np.where(prediction[i]==max(prediction[i])))\n",
    "    acc=0\n",
    "    \n",
    "    for x in range (len(all_prediction_index)):\n",
    "        \n",
    "        if all_prediction_index[x][0][0] == int (y_var[x]):\n",
    "            acc +=1\n",
    "    return acc/len(all_prediction_index)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and get train and test datasets and targets. Also change vowel targets into integers that represent each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 0, 3,  ..., 5, 6, 7])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://raw.githubusercontent.com/efosler/cse5522data/master/vowelfmts.csv'\n",
    "df=pd.read_csv(url)\n",
    "targets = {'iy':0, 'ih':1, 'ey':2, 'eh':3, 'ah':4, 'ao':5, 'ow':6, 'uw':7, 'ax':8 }\n",
    "for i in range (len(df['vowel'])):\n",
    "    df.at[i, 'vowel'] = targets[df['vowel'][i]]\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(df[['f1','f2']],df[['vowel']])\n",
    "train_data = np.array(train_data).astype(float)\n",
    "scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "train_data = scaler.transform(train_data)   \n",
    "test_data = np.array(test_data).astype(float)\n",
    "test_data = scaler.transform(test_data)   \n",
    "train_targets = np.array(train_targets).astype(int)\n",
    "test_targets = np.array(test_targets).astype(int)\n",
    "list_test_targets=[]\n",
    "list_train_targets=[]\n",
    "for i in range (len(test_targets)):\n",
    "    list_test_targets.append(test_targets[i][0])\n",
    "for x in range (len(train_targets)):\n",
    "    list_train_targets.append(train_targets[x][0])\n",
    "data_train = torch.tensor(train_data)\n",
    "data_test = torch.tensor(test_data)\n",
    "target_train = torch.tensor(list_train_targets)\n",
    "target_test = torch.tensor(list_test_targets)\n",
    "\n",
    "target_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 9.85786247253418, \n",
      "Epoch: 1000, Loss: 1.430095911026001, \n",
      "Epoch: 2000, Loss: 1.3031542301177979, \n",
      "Epoch: 3000, Loss: 1.2640380859375, \n",
      "Epoch: 4000, Loss: 1.240235686302185, \n",
      "Epoch: 5000, Loss: 1.2225170135498047, \n",
      "Epoch: 6000, Loss: 1.208197832107544, \n",
      "Epoch: 7000, Loss: 1.1960877180099487, \n",
      "Epoch: 8000, Loss: 1.185548186302185, \n",
      "Epoch: 9000, Loss: 1.1761884689331055, \n",
      "Epoch: 10000, Loss: 1.167763590812683, \n",
      "Epoch: 11000, Loss: 1.1600896120071411, \n",
      "Epoch: 12000, Loss: 1.1530475616455078, \n",
      "Epoch: 13000, Loss: 1.1465362310409546, \n",
      "Epoch: 14000, Loss: 1.1404814720153809, \n",
      "Epoch: 15000, Loss: 1.1348258256912231, \n",
      "Epoch: 16000, Loss: 1.1295231580734253, \n",
      "Epoch: 17000, Loss: 1.12453293800354, \n",
      "Epoch: 18000, Loss: 1.1198209524154663, \n",
      "Epoch: 19000, Loss: 1.1153600215911865, \n",
      "Epoch: 20000, Loss: 1.1111290454864502, \n",
      "Epoch: 21000, Loss: 1.1071083545684814, \n",
      "Epoch: 22000, Loss: 1.1032763719558716, \n",
      "Epoch: 23000, Loss: 1.0996222496032715, \n",
      "Epoch: 24000, Loss: 1.0961283445358276, \n",
      "Epoch: 25000, Loss: 1.0927828550338745, \n",
      "Epoch: 26000, Loss: 1.0895801782608032, \n",
      "Epoch: 27000, Loss: 1.086504578590393, \n",
      "Epoch: 28000, Loss: 1.0835524797439575, \n",
      "Epoch: 29000, Loss: 1.0807149410247803, \n",
      "Epoch: 30000, Loss: 1.0779790878295898, \n",
      "Epoch: 31000, Loss: 1.0753438472747803, \n",
      "Epoch: 32000, Loss: 1.0728025436401367, \n",
      "Epoch: 33000, Loss: 1.0703487396240234, \n",
      "Epoch: 34000, Loss: 1.067976713180542, \n",
      "Epoch: 35000, Loss: 1.0656827688217163, \n",
      "Epoch: 36000, Loss: 1.0634605884552002, \n",
      "Epoch: 37000, Loss: 1.0613099336624146, \n",
      "Epoch: 38000, Loss: 1.05922532081604, \n",
      "Epoch: 39000, Loss: 1.0572004318237305, \n",
      "Epoch: 40000, Loss: 1.0552345514297485, \n",
      "Epoch: 41000, Loss: 1.053324818611145, \n",
      "Epoch: 42000, Loss: 1.051466703414917, \n",
      "Epoch: 43000, Loss: 1.0496561527252197, \n",
      "Epoch: 44000, Loss: 1.0478969812393188, \n",
      "Epoch: 45000, Loss: 1.0461786985397339, \n",
      "Epoch: 46000, Loss: 1.044503092765808, \n",
      "Epoch: 47000, Loss: 1.042871117591858, \n",
      "Epoch: 48000, Loss: 1.0412755012512207, \n",
      "Epoch: 49000, Loss: 1.0397157669067383, \n"
     ]
    }
   ],
   "source": [
    "inputDimension = 2\n",
    "hiddenDimension = 100\n",
    "outputDimension = 9\n",
    "model1 = train(data_train,target_train,50000,inputDimension, hiddenDimension, outputDimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on test dataset and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6160493827160494\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(data_test,target_test,model1)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "### Let hidden units of MLP model be 200 this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 8.498006820678711, \n",
      "Epoch: 1000, Loss: 1.2580021619796753, \n",
      "Epoch: 2000, Loss: 1.208699345588684, \n",
      "Epoch: 3000, Loss: 1.181119441986084, \n",
      "Epoch: 4000, Loss: 1.1609309911727905, \n",
      "Epoch: 5000, Loss: 1.1447937488555908, \n",
      "Epoch: 6000, Loss: 1.1313188076019287, \n",
      "Epoch: 7000, Loss: 1.119780421257019, \n",
      "Epoch: 8000, Loss: 1.1097279787063599, \n",
      "Epoch: 9000, Loss: 1.1008610725402832, \n",
      "Epoch: 10000, Loss: 1.0929547548294067, \n",
      "Epoch: 11000, Loss: 1.0858397483825684, \n",
      "Epoch: 12000, Loss: 1.0793893337249756, \n",
      "Epoch: 13000, Loss: 1.0735032558441162, \n",
      "Epoch: 14000, Loss: 1.0680941343307495, \n",
      "Epoch: 15000, Loss: 1.0631028413772583, \n",
      "Epoch: 16000, Loss: 1.0584731101989746, \n",
      "Epoch: 17000, Loss: 1.054162621498108, \n",
      "Epoch: 18000, Loss: 1.0501347780227661, \n",
      "Epoch: 19000, Loss: 1.0463553667068481, \n",
      "Epoch: 20000, Loss: 1.0428001880645752, \n",
      "Epoch: 21000, Loss: 1.0394481420516968, \n",
      "Epoch: 22000, Loss: 1.0362788438796997, \n",
      "Epoch: 23000, Loss: 1.0332729816436768, \n",
      "Epoch: 24000, Loss: 1.030421495437622, \n",
      "Epoch: 25000, Loss: 1.0277061462402344, \n",
      "Epoch: 26000, Loss: 1.0251160860061646, \n",
      "Epoch: 27000, Loss: 1.022641897201538, \n",
      "Epoch: 28000, Loss: 1.020277500152588, \n",
      "Epoch: 29000, Loss: 1.0180097818374634, \n",
      "Epoch: 30000, Loss: 1.0158365964889526, \n",
      "Epoch: 31000, Loss: 1.013746976852417, \n",
      "Epoch: 32000, Loss: 1.0117343664169312, \n",
      "Epoch: 33000, Loss: 1.0097992420196533, \n",
      "Epoch: 34000, Loss: 1.0079309940338135, \n",
      "Epoch: 35000, Loss: 1.0061289072036743, \n",
      "Epoch: 36000, Loss: 1.004386305809021, \n",
      "Epoch: 37000, Loss: 1.0026986598968506, \n",
      "Epoch: 38000, Loss: 1.0010663270950317, \n",
      "Epoch: 39000, Loss: 0.9994847774505615, \n",
      "Epoch: 40000, Loss: 0.9979494214057922, \n",
      "Epoch: 41000, Loss: 0.9964587092399597, \n",
      "Epoch: 42000, Loss: 0.9950107336044312, \n",
      "Epoch: 43000, Loss: 0.9936039447784424, \n",
      "Epoch: 44000, Loss: 0.9922335743904114, \n",
      "Epoch: 45000, Loss: 0.9909012913703918, \n",
      "Epoch: 46000, Loss: 0.9895997047424316, \n",
      "Epoch: 47000, Loss: 0.9883330464363098, \n",
      "Epoch: 48000, Loss: 0.9870991706848145, \n",
      "Epoch: 49000, Loss: 0.9858912229537964, \n"
     ]
    }
   ],
   "source": [
    "hiddenDimension2 = 200\n",
    "model2 = train(data_train,target_train,50000,inputDimension, hiddenDimension2, outputDimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6407407407407407\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(data_test,target_test,model2)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a three layer multilevel perceptron with same 100 hidden units each layer. It is the second variation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP3(nn.Module):\n",
    "    def __init__(self,input_dim, hid_dim1, hid_dim2,output_dim):\n",
    "        self.input_dim=input_dim\n",
    "        self.hid_dim1=hid_dim1\n",
    "        self.hid_dim2=hid_dim2\n",
    "        self.output_dim=output_dim\n",
    "        # initialze the superclass\n",
    "        super(MLP3, self).__init__()\n",
    "        # this will create a linear layer with input_dim x hid_dim parameters\n",
    "        # in addition to a bias unit with hid_dim parameters\n",
    "        self.lin1 = nn.Linear(input_dim, hid_dim1)\n",
    "        self.lin2 = nn.Linear(hid_dim1, hid_dim2)\n",
    "        # same thing here except hid_dim x output_dim\n",
    "        self.lin3 = nn.Linear(hid_dim2, output_dim)\n",
    "    \n",
    "    # this is where the meat of the action is\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)  # linear combination of inputs\n",
    "        x = torch.sigmoid(x) # then through sigmoid - output of first layer\n",
    "        x = self.lin2(x) # linear combination of hidden units\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWithMLP3(data, target, epoch, inputDim, hiddenDim1, hiddenDim2, outputDim):\n",
    "    model = MLP3(inputDim, hiddenDim1, hiddenDim2, outputDim)\n",
    "    weights_init(model)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0006, momentum=0.9)\n",
    "    for i in range(epoch):\n",
    "        x_var = Variable(data, requires_grad=False)\n",
    "        y_var = Variable(target, requires_grad=False)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x_var.float())\n",
    "        y_var = y_var.long()\n",
    "        loss = loss_func(y_hat, y_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Epoch: {0}, Loss: {1}, \".format(i, loss.data.numpy()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 11.70850658416748, \n",
      "Epoch: 1000, Loss: 1.065604567527771, \n",
      "Epoch: 2000, Loss: 1.0116064548492432, \n",
      "Epoch: 3000, Loss: 0.988967776298523, \n",
      "Epoch: 4000, Loss: 0.9745367169380188, \n",
      "Epoch: 5000, Loss: 0.9638038873672485, \n",
      "Epoch: 6000, Loss: 0.9551545977592468, \n",
      "Epoch: 7000, Loss: 0.9479348659515381, \n",
      "Epoch: 8000, Loss: 0.9418618679046631, \n",
      "Epoch: 9000, Loss: 0.9366999864578247, \n",
      "Epoch: 10000, Loss: 0.9322539567947388, \n",
      "Epoch: 11000, Loss: 0.9284011125564575, \n",
      "Epoch: 12000, Loss: 0.9250258803367615, \n",
      "Epoch: 13000, Loss: 0.9220333099365234, \n",
      "Epoch: 14000, Loss: 0.9193418622016907, \n",
      "Epoch: 15000, Loss: 0.916888952255249, \n",
      "Epoch: 16000, Loss: 0.9146325588226318, \n",
      "Epoch: 17000, Loss: 0.9125417470932007, \n",
      "Epoch: 18000, Loss: 0.9105932116508484, \n",
      "Epoch: 19000, Loss: 0.9087673425674438, \n",
      "Epoch: 20000, Loss: 0.9070479869842529, \n",
      "Epoch: 21000, Loss: 0.9054270386695862, \n",
      "Epoch: 22000, Loss: 0.9038897752761841, \n",
      "Epoch: 23000, Loss: 0.9024304747581482, \n",
      "Epoch: 24000, Loss: 0.9010399580001831, \n",
      "Epoch: 25000, Loss: 0.8997114896774292, \n",
      "Epoch: 26000, Loss: 0.8984408974647522, \n",
      "Epoch: 27000, Loss: 0.8972219228744507, \n",
      "Epoch: 28000, Loss: 0.896050214767456, \n",
      "Epoch: 29000, Loss: 0.894923746585846, \n",
      "Epoch: 30000, Loss: 0.8938391804695129, \n",
      "Epoch: 31000, Loss: 0.8927901983261108, \n",
      "Epoch: 32000, Loss: 0.8917756080627441, \n",
      "Epoch: 33000, Loss: 0.8907929062843323, \n",
      "Epoch: 34000, Loss: 0.8898412585258484, \n",
      "Epoch: 35000, Loss: 0.8889172077178955, \n",
      "Epoch: 36000, Loss: 0.8880178332328796, \n",
      "Epoch: 37000, Loss: 0.8871443867683411, \n",
      "Epoch: 38000, Loss: 0.8862928152084351, \n",
      "Epoch: 39000, Loss: 0.885461151599884, \n",
      "Epoch: 40000, Loss: 0.8846497535705566, \n",
      "Epoch: 41000, Loss: 0.8838573694229126, \n",
      "Epoch: 42000, Loss: 0.8830817937850952, \n",
      "Epoch: 43000, Loss: 0.8823259472846985, \n",
      "Epoch: 44000, Loss: 0.8815868496894836, \n",
      "Epoch: 45000, Loss: 0.8808658123016357, \n",
      "Epoch: 46000, Loss: 0.8801611661911011, \n",
      "Epoch: 47000, Loss: 0.8794723749160767, \n",
      "Epoch: 48000, Loss: 0.8788026571273804, \n",
      "Epoch: 49000, Loss: 0.8781473636627197, \n"
     ]
    }
   ],
   "source": [
    "inputDimension = 2\n",
    "hiddenDimension1 = 100\n",
    "hiddenDimenstion2 = 100\n",
    "outputDimension = 9\n",
    "model3 = trainWithMLP3(data_train,target_train,50000,inputDimension, hiddenDimension1, hiddenDimension2, outputDimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.662962962962963\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(data_test,target_test,model3)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of three different model:\n",
    "    First model: Loss: 1.0397157669067383, Accuracy:  0.6160493827160494\n",
    "    Second model: Loss: 0.9858912229537964, Accuracy:  0.6407407407407407\n",
    "    Third model: Loss: 0.8781473636627197, Accuracy:  0.662962962962963\n",
    "    \n",
    "    First model is the model in Part 2, 2 layer Perceptron with 100 hidden units.\n",
    "    Second model is the model with a 2 layer Perceptron with 200 hidden units.\n",
    "    Third model is the model with a 3 layer Perceptron with 100 hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"red\"> Answer to question: </font> Create a short writeup explaining (1) why you chose the variations, and (2) how it affected performance on the test set.\n",
    "    Since the input dimension is always 2 (\"f1\", \"f2\") and output dimension is always 9 (There are 9 vowels), so the things we can change in model is limited. Changing hidden layers, hidden units or functions. Here are reasons why I chose the variations.\n",
    "    (1) Compare the second model with the first model, I added 100 hidden units. In the text book, it said that appropriate hidden neurons with increase the performance of model. I wanted to try the affect of hidden units in this dataset. So I added 100 hidden units. \n",
    "    \n",
    "        Compare the third model with the first model, I added a hidden layer and kept the same number of hidden units. As I know in deeplearning, increasing the number of hidden layers might improve the accuracy or might not, it really depends on the complexity of the problem that you are trying to solve. I wanted to see the result of adding just one layer in this model, and whether it improved the performance or not.\n",
    "      \n",
    "    (2) Compare the second model with the first model, the loss in training droped 0.05 and the accuracy in test set increased 0.03. So, adding 100 hidden units increased the performance of the model.\n",
    "    \n",
    "        Compare the third model with the first model, the loss in trainning droped 0.16 and the accuracy in test set increased 0.05. So, adding 1 hidden layer increase the performance of the model, and the it affect more than just adding 100 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
